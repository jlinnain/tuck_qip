{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>Quantitative Investing with Python</span>\n",
    "\n",
    "### Professor Juhani Linnainmaa\n",
    "\n",
    "Dartmouth College and Kepos Capital\n",
    "\n",
    "*Last revised:* January 13, 2025\n",
    "\n",
    "--- \n",
    "\n",
    "# **Topic 3:** CRSP Data and Short-Term Reversals\n",
    "\n",
    "The goal in this section of the course is to **get familiarity with constructing trading strategies by replicating some academic factors**\n",
    "\n",
    "I will use monthly CRSP data -- CRSP stands for the Center for Research in Security Prices.\n",
    "- Most universities and colleges subscribe to data such as CRSP through Wharton's WRDS service\n",
    "- I'm providing a part of the monthly CRSP file from May 1962 through September 2023\n",
    "  - I include, at random, 25% of stocks that have at least 1 year of returns anywhere during the sample period\n",
    "  \n",
    "I will first consider \"price-based\" factors, that is, trading rules that are based only past security price information\n",
    "\n",
    "The major price-based factors include\n",
    "- Size\n",
    "- Short- and long-term reversals\n",
    "- Momentum\n",
    "- Idiosyncratic volatility\n",
    "- Betting against beta\n",
    "\n",
    "A **factor** is just a trading rule. It specifies the rule that determines what you will buy and sell.\n",
    "\n",
    "When you construct a factor -- as discussed in Lecture 1 -- you need to make *many* choices \n",
    "- Moreover, the data may change over time and so, in practice, it is *very* difficult to replicate a factor perfectly unless you have the original data and code\n",
    "- Sometimes the original papers (and industry reports) do not provide enough details for replicating the factors\n",
    "  - For example, Li, Novy-Marx, and Velikov (2019) (https://cfr.pub/published/papers/li2020liquidity.pdf) struggled to replicate a famous factor paper until they figured out that the authors had used an unreported rule:\n",
    "  \n",
    "  \n",
    ">  \"Finally, while not noted by PS, they delete zero-volume observations when estimating Eq. (1), and doing so here is crucial to generating a high correspondence between our results and those reported in their paper.\n",
    "  \n",
    "> Determining this fact required implementing numerous variations on the methodology described in PS. This involved labor far beyond what could reasonably be expected for casual replication, and was only possible because of the public aggregate liquidity series maintained by PS, which allowed us to infer which variations were important for generating a close correspondence.\" (p. 227)\n",
    "\n",
    "In this notebook I only construct a <span style='color:red'>**short-term reversals**</span> factor\n",
    "\n",
    "- This well-known strategy is based on the finding that stock returns display reversals at one-month horizons\n",
    "  - Stocks that go up the most in one month tend to have low returns returns relative to other stocks the *next month*\n",
    "  - In the academic literature the typical reference to this finding is Jegadeesh (1990): https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1990.tb05110.x\n",
    "  - Narasimhan Jegadeesh is even better known for publishing the famous 'momentum' paper three years later with Sheridan Titman<br><br>\n",
    "  \n",
    "- I first construct the strategy using \"linear portfolio\" weights to keep it simple\n",
    "- I then replicate the Fama-French approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from io import BytesIO, StringIO\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"text-align: center; font-family: 'Georgia', sans-serif; font-size: 48px; font-weight: bold; color: red;\">\n",
    "    Read and process monthly stock data from CRSP\n",
    "</div>\n",
    "\n",
    "- I downloaded the CRSP data from WRDS, zipped it, and put into Dropbox\n",
    "  - This is what the raw data look like\n",
    "- The code below downloads the Dropbox file and unzips it into a DataFrame\n",
    "- **There are lots of pre-processing steps below. We need to do this so that the data are usable.**\n",
    "- Once I'm done, I'll save the processed file into JHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp_url = 'https://dl.dropboxusercontent.com//scl/fi/xuextjrshyajiav4wlj9b/CRSP.csv.zip?rlkey=6x8drx3htj72v7qhcppc19u5w'\n",
    "response = requests.get(crsp_url)\n",
    "with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "    with z.open('CRSP.csv') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- I don't really need company names, but I'll put them into a Series just in case I want to look something up based on PERMNO\n",
    "- I also don't need TICKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = df[['permno','comnam']]\n",
    "company_names = company_names.groupby('permno').last().squeeze()\n",
    "df = df.drop(columns=['comnam','ticker'])\n",
    "company_names.name = 'Company names'\n",
    "company_names.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing 2\n",
    "\n",
    "1. change dates to datetime and convert them to be as of the 1st of month\n",
    "2. set permno-date as the index (it becomes a multi-index)\n",
    "3. the typical universe in equities is to keep common stock traded on NYSE, Nasdaq, and AMEX -- filter based on SHRCD and EXCHCD\n",
    "4. drop SHRCD and EXCHCD because we don't need them anymore\n",
    "5. convert two returns variable, DLRET and RET, into floats\n",
    "   - there are some strings, which we ignore (this is the 'coerce' argument)\n",
    "6. compute returns inclusive of delisting returns\n",
    "   - our return variable is either 'normal return', 'delisting return', or, if both exist, the compounded return\n",
    "   - we need to be careful with missign values - I'm filling in zeros for NaNs in the computation but then, if neither return exists, putting them back in  \n",
    "7. compute market capitalization in millions\n",
    "   - note that PRC is negative to indicate that it is the spread midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data variable and put permno and date in the index\n",
    "df['date'] = pd.to_datetime(df['date'], format='%d%b%Y').dt.to_period('M').dt.to_timestamp()\n",
    "df = df.set_index(['permno', 'date']).sort_index()\n",
    "\n",
    "# we want to keep SHRCD = 10 or 11 and EXCHCD=1,2, or 3 -- the raw input data should already have these filters\n",
    "df = df[(df['shrcd'].isin([10,11])) & (df['exchcd'].isin([1,2,3]))]\n",
    "\n",
    "# drop the SHRCD variable - we don't need it anymore\n",
    "df = df.drop(columns=['shrcd'])\n",
    "\n",
    "# what *is* DLRET?\n",
    "print(df['dlret'].describe())\n",
    "\n",
    "col_list = ['dlret', 'ret']\n",
    "for col in col_list:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "df['ret2'] = (1 + df['ret'].fillna(0)) * (1 + df['dlret'].fillna(0)) - 1\n",
    "\n",
    "neither_return_exists = (df['ret'].isnull()) & (df['dlret'].isnull())\n",
    "df.loc[neither_return_exists, 'ret2'] = np.nan\n",
    "\n",
    "# drop the original return variables - we don't need them anymore\n",
    "df = df.drop(columns=['ret', 'dlret'])\n",
    "df = df.rename(columns={'ret2': 'ret'})\n",
    "\n",
    "# compute market cap in millions\n",
    "df['me'] = np.abs(df['prc']) * df['shrout'] / 1_000\n",
    "df['me'] = df['me'].replace({0: np.nan}) # Shares outstanding is sometimes zero or PRC is missing -> set me to missing\n",
    "df = df.drop(columns=['prc', 'shrout']) # Drop PRC -> we don't need it anymore\n",
    "\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are a few duplicate observations: same permno and month\n",
    "\n",
    "- Let's get rid of them by averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Is the index unique? {df.index.is_unique}')\n",
    "df = df.groupby(level=['permno', 'date']).mean()\n",
    "print(f'Is the index now unique? {df.index.is_unique}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which observations have the biggest market capitalization in our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('me', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which stock had the market cap of \\\\$513,362M in September 2007?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names.loc[11850]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save finished file into a pickle (```PKL```) file\n",
    "\n",
    "- Be careful with pickle files - they are very convenient but not efficient and break between Python and Pandas versions!\n",
    "\n",
    "Let me try to figure out the full path to the root directory; it has a weird name; for me it is ```/home/jovyan```\n",
    "\n",
    "- I want to save the data in a ```data``` directory below the root\n",
    "- ```os``` package has all kinds of functions for dealing with the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "print(f'Current directory: {current_dir}')\n",
    "print(f'Parent directory: {parent_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory and filename (CHANGE AS NEEDED!)\n",
    "directory = '/home/jovyan/data'\n",
    "filename = 'crsp.pkl'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame as a pickle file\n",
    "file_path = os.path.join(directory, filename)\n",
    "df.to_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"text-align: center; font-family: 'Georgia', sans-serif; font-size: 48px; font-weight: bold; color: red;\">\n",
    "    Short-Term Reversals Strategy\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading strategies\n",
    "\n",
    "A trading strategy is a systematic rule based on data we know at the time we make the trading decisions\n",
    "\n",
    "Your strategy could be **value investing:**\n",
    "\n",
    "1. Buy value stocks and sell growth stocks (this is known as a long-short portfolio)\n",
    "2. Look at the data once a month to rebalance your portfolios as stocks' characteristics change\n",
    "\n",
    "### Strategy 1: Short-term reversals\n",
    "\n",
    "**Short-term reversals** is the empirical finding that, at short horizons, stock returns tend **reverse**\n",
    "\n",
    "In monthly data, a strategy that trades short-term reversals is simple:\n",
    "\n",
    "- Rank stocks by their prior-month returns (e.g., their returns in December)\n",
    "- At the end of the month, buy stocks with the lowest returns and sell stocks with the highest returns\n",
    "\n",
    "To get started, I'll create the following strategy:\n",
    "\n",
    "- Every month rank stocks based on their one-month returns\n",
    "- Buy stocks with the lowest returns and sell those with the highest returns\n",
    "\n",
    "We need to pay some attention to timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute portfolio weights\n",
    "\n",
    "- We want to invest the most in stocks with the most positive returns and short the most those with the most negative returns\n",
    "- I'll rank stocks each month based on returns. \n",
    "  - ```rank(pct=True)``` returns a percentile rank; it runs from 0 to 1\n",
    "  - If I subtract 0.5 and multiply by -2, I get the desired weights\n",
    "  - *Note:* The scale of my weights here is arbitrary. It won't make sense to invest -100% to one stock and 95% in another -- I'll address this issue later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the data\n",
    "\n",
    "df = pd.read_pickle('/home/jovyan/data/crsp.pkl')\n",
    "\n",
    "df['position'] = (\n",
    "    df.groupby(level='date')['ret']  # Group by the 'date' level in the index\n",
    "    .transform(lambda x: (x.rank(pct=True) - 0.5) * -2)  # Apply percentile rank transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute portfolio return\n",
    "\n",
    "- Let me keep this simple and create two different dataframes:\n",
    "\n",
    "1. ```Position``` will have the weights we just computed\n",
    "2. ```Ret``` will have the asset returns\n",
    "\n",
    "I reshape these so that we have ```date``` in the index and ```permno``` in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = df['position'].unstack(level='permno')\n",
    "ret = df['ret'].unstack(level='permno')\n",
    "\n",
    "display('position:', position.tail(5))\n",
    "display('ret:', ret.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember** that, at the moment, position on the \"November 2022\" row is based on November 2022 returns\n",
    "\n",
    "- We want to take this position at the end of November 2022 and earn the return on this position in December 2022\n",
    "- So we need to **shift** weights back by one month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_position = position.shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The return on an asset is just the product of weights and returns\n",
    "\n",
    "- Once we have those products, we can just take the sum across all assets to get the total return per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_return = lagged_position.mul(ret).sum(axis=1, min_count=1)\n",
    "portfolio_return.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_return.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These don't look like returns because our weights are far too large (we are investing -100% in the stock with the highest return and +100% in the stock with the lowest return\n",
    "\n",
    "- I can figure out how to scale these weights by looking at these returns\n",
    "- For example, if it looks like the portfolio has realized a volatility of 100% and I want to target a volatility of 15%, I can just multiple the weights by (15% / 100%)\n",
    "- What I do is, I look at the realized volatility over the prior 12 months (up to one month ago) and use that to target volatility\n",
    "  - There is no lookahead bias\n",
    "- I'll call the resulting strategy ```strev```, which is the typical abbreviation for short-term reversals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realized_vol = np.sqrt(12) * portfolio_return.rolling(window=12, min_periods=3).std()\n",
    "scaled_position = 0.15 * position.div(realized_vol.shift(1), axis=0) \n",
    "scaled_lagged_position = scaled_position.shift(1)\n",
    "\n",
    "strev = scaled_lagged_position.mul(ret).sum(axis=1, min_count=1)\n",
    "strev.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much volatility do we realize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strev.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.057 * np.sqrt(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the strategy's performance\n",
    "\n",
    "I define a helper function for doing some analysis\n",
    "- It just means that I don't have to rewrite the same code\n",
    "- It is good to write modular code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_returns(r=None, name=None, start_date='1964-01', end_date='2023-09'):\n",
    "    r = r.loc[start_date:end_date]\n",
    "    ir = np.sqrt(12) * r.mean() / r.std()\n",
    "    print(f'Analysis of a strategy: \"{name}\"')\n",
    "    print(f'Start: {start_date}, End: {end_date}')\n",
    "    print(f'Sharpe ratio: {ir:.2f}')\n",
    "    r.cumsum().plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_returns(strev, 'Short-term reversals (linear weights)', end_date='1995-12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Fama and French's computation\n",
    "\n",
    "- We want to make sure that we did this (approx.) right\n",
    "  - As we will see, there is a small discrepancy\n",
    "\n",
    "### Get Fama-French factors from Ken French's website \n",
    "\n",
    "- I write a helper function ```download_french_data``` that I can use to download the data\n",
    "- There are some file-specific issues that I need to control for \n",
    "  - In the French data files, there are varying amounts of additional lines of text at the top and the same file often has multiple datasets (e.g., monthly and annual factors)\n",
    "    - In my code below, I lightly control for these issues at least for the two files I want to download\n",
    "- I get both Fama-French factors and portfolios formed based on short-term reversals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_french_data(url=None, csvname=None, skiplines=None):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "    # If the request is NOT successful, raise an exception\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to download zip file. Status code: {response.status_code}\")\n",
    "\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zip_file:\n",
    "\n",
    "        # Check if the file exists in the zip archive\n",
    "        if csvname in zip_file.namelist():\n",
    "            # Read the CSV file directly from the zip archive\n",
    "            with zip_file.open(csvname) as csv_file:\n",
    "                lines = csv_file.readlines()\n",
    "\n",
    "            # Remove rows from the beginning\n",
    "            lines = lines[skiplines:]\n",
    "\n",
    "            # Create a DataFrame from the trimmed lines using StringIO\n",
    "            # First need to decode byte strings into unicode\n",
    "            lines = [line.decode(\"utf-8\") for line in lines]\n",
    "\n",
    "            # at some point the file switches from monthly factors to annual factors and other stuff\n",
    "            # we can delete what ever comes after\n",
    "            for idx, line in enumerate(lines):\n",
    "                if ('Annual Factors' in line) or (len(line.strip())==0): break\n",
    "                \n",
    "            lines = lines[:idx]\n",
    "            clean_csv = '\\n'.join(lines)\n",
    "            df = pd.read_csv(StringIO(clean_csv))   \n",
    "            \n",
    "            # convert date into a format we understand and make it the index\n",
    "            # also convert returns from percentages (e.g., 2.12) to decimles (e.g., 0.0212) by dividing by 100\n",
    "            df['date'] = df['Unnamed: 0'].apply(lambda x: datetime.strptime(str(x), '%Y%m'))\n",
    "            df = df.drop(columns='Unnamed: 0')\n",
    "            df = df.set_index('date') / 100\n",
    "\n",
    "            print(f'File {csvname} read successfully!')\n",
    "            return df\n",
    "        else:\n",
    "            print(f'Zip file found but file {csvname} not found in the archive.')   \n",
    "            return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read FF5 factors and clean the data\n",
    "\n",
    "- Convert returns to decimals and date from YYYYMM to datatime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file we want to read -- the CSV file inside has almost the same name \n",
    "url = 'https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_CSV.zip'\n",
    "csvname = 'F-F_Research_Data_5_Factors_2x3.csv'\n",
    "\n",
    "ff_data = download_french_data(url=url, csvname=csvname, skiplines=3)\n",
    "ff_data.to_pickle('data/ff_data.pkl')\n",
    "\n",
    "print('\\nData:\\n')\n",
    "print(ff_data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read returns on decile portfolios formed based on short-term reversals\n",
    "\n",
    "- This is similar to what we were doing above, except that French reports returns for all ten portfolios\n",
    "  - We'll take them all and then compute the return on 'losers' minus 'winners'\n",
    "- Similar to above, after we get the raw data, I clean it a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_ST_Reversal_Factor_CSV.zip'\n",
    "csvname = 'F-F_ST_Reversal_Factor.CSV'    \n",
    "\n",
    "strev_ff = download_french_data(url=url, csvname=csvname, skiplines=13)\n",
    "\n",
    "print('\\nData after processing:\\n')\n",
    "print(strev_ff.head(3))\n",
    "\n",
    "# convert to a Series (it was a DataFrame)\n",
    "strev_ff = strev_ff.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_returns(strev_ff, 'Short-term reversals (FF version)', end_date='1995-12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between our strategy and that of Fama and French\n",
    "\n",
    "- There is the small issue that the dates are different\n",
    "- I change them to monthly so I can merge the two series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = strev.copy()\n",
    "ours.index = ours.index.to_period('M').to_timestamp('M')\n",
    "ours.name = 'Our strategy'\n",
    "\n",
    "theirs = strev_ff.copy()\n",
    "theirs.index = theirs.index.to_period('M').to_timestamp('M')\n",
    "theirs.name = 'FF\\'s strategy'\n",
    "pd.concat([ours, theirs], axis=1).corr().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:red'> **Note:**</span> Our replication does not match the Fama-French return exactly\n",
    "\n",
    "**Question: What is the difference?**\n",
    "\n",
    "We are doing a few things differently:\n",
    "\n",
    "1. Fama and French construct their strategy by sorting stocks into six portfolio, making these portfolios value-weighted, and then are long and short these portfolios\n",
    "   - Fama and French also put their stocks into portfolios using \"NYSE breakpoints\" \n",
    "2. Our weights are proportional to realized returns\n",
    "3. Fama and French include all stocks; we include just 25% selected at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"text-align: center; font-family: 'Georgia', sans-serif; font-size: 48px; font-weight: bold; color: red;\">\n",
    "    A replication of Fama and French's factor\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/home/jovyan/data/crsp.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute the NYSE breakpoints for size and 1-month return\n",
    "   \n",
    "- 50th percentile for market cap (size)\n",
    "- 30th and 70th percentiles for return\n",
    "\n",
    "**Note:** I keep only ```exchcd==1``` observations in the sample and then group by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = df.loc[df['exchcd']==1]['me'].dropna().groupby(level='date')\n",
    "me_p50 = grp.apply(lambda x: np.percentile(x, 10))\n",
    "\n",
    "grp = df.loc[df['exchcd']==1]['ret'].dropna().groupby(level='date')\n",
    "ret_p30 = grp.apply(lambda x: np.percentile(x, 30))\n",
    "ret_p70 = grp.apply(lambda x: np.percentile(x, 70))\n",
    "\n",
    "breakpoints = pd.DataFrame(\n",
    "    {'me_p50': me_p50,\n",
    "     'ret_p30': ret_p30, \n",
    "     'ret_p70': ret_p70,}\n",
    ")\n",
    "\n",
    "breakpoints.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge breakpoints back into our original dataframe\n",
    "\n",
    "- We need to specify what we are merging on.\n",
    "- On the \"left\" we are merging by (level) \"date\"; on the \"right\" we are merging by the index (which is also date)\n",
    "- We also need to specify what observations we want to keep: those on the left, those on the right, the union (inner), or the join (outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(breakpoints, left_on='date', right_index=True, how='left')\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign stocks into portfolios based on where they fall based on the breakpoints\n",
    "\n",
    "- For Fama and French, we only need the losers (<= 30th percentile) and winners (> 70th percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['portfolio'] = '' # initialize the column (not super-necessary)\n",
    "df.loc[(df['ret'] <= df['ret_p30']) & (df['me'] <= df['me_p50']), 'portfolio'] = 'small_losers'\n",
    "df.loc[(df['ret'] <= df['ret_p30']) & (df['me'] >  df['me_p50']), 'portfolio'] = 'big_losers'\n",
    "df.loc[(df['ret'] >  df['ret_p70']) & (df['me'] <= df['me_p50']), 'portfolio'] = 'small_winners'\n",
    "df.loc[(df['ret'] >  df['ret_p70']) & (df['me'] >  df['me_p50']), 'portfolio'] = 'big_winners'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute returns for value-weighted portfolios\n",
    "\n",
    "- Let's pretend that I invest 'me' into each stock at the end of the month\n",
    "- If I want to compute the return on the portfolio the next month, it'll be something like ```sum(me * return) / sum(me)``` -- how much money I made divided by the total amount I invested\n",
    "- To get the March 1986 return, I need to get portfolio assignments and market caps from February 1986\n",
    "  - This is the timing stuff again\n",
    "  - **Important:** If I have multiples levels in the index, ```shift()``` wouldn't probably do what you want\n",
    "  - It would just take the value from the previous row -- might be for a different stock\n",
    "  - Do make sure that I grab the right return, I need to ```groupby``` permno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lme'] = df.groupby('permno')['me'].shift(1)\n",
    "df['Lportfolio'] = df.groupby('permno')['portfolio'].shift(1)\n",
    "\n",
    "df['Lme_x_ret'] = df['Lme'].mul(df['ret'])\n",
    "\n",
    "sums = df.reset_index().groupby(['date', 'Lportfolio'])[['Lme', 'Lme_x_ret']].sum(min_count=1)\n",
    "portfolio_returns = sums['Lme_x_ret'].div(sums['Lme']).unstack(level='Lportfolio')\n",
    "portfolio_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the return for the Fama-French-style factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strev_vw = (\n",
    "    (1/2) * (portfolio_returns['small_losers'] + portfolio_returns['big_losers']) -\n",
    "    (1/2) * (portfolio_returns['small_winners'] + portfolio_returns['big_winners'])\n",
    ")\n",
    "\n",
    "analyze_returns(strev_vw, 'Short-term reversals (our FF replication)', end_date='1995-12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between the three strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = strev.copy()\n",
    "ours.index = ours.index.to_period('M').to_timestamp('M')\n",
    "ours.name = 'Linear'\n",
    "\n",
    "our_vw = strev_vw.copy()\n",
    "our_vw.index = our_vw.index.to_period('M').to_timestamp('M')\n",
    "our_vw.name = 'VW'\n",
    "\n",
    "theirs = strev_ff.copy()\n",
    "theirs.index = theirs.index.to_period('M').to_timestamp('M')\n",
    "theirs.name = 'FF'\n",
    "pd.concat([ours, our_vw, theirs], axis=1).corr().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Note:** The remaining difference is due to the fact that we have only 25% of the universe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
