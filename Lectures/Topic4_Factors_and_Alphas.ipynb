{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>Quantitative Investing with Python</span>\n",
    "\n",
    "### Professor Juhani Linnainmaa\n",
    "\n",
    "Dartmouth College and Kepos Capital (Co-Director of Research)\n",
    "\n",
    "*Last revised:* January 24, 2024\n",
    "\n",
    "Changes:\n",
    "\n",
    "- A more elegant and robust implementation for the process of matching CRSP and Compustat (1/24/2024)\n",
    "\n",
    "--- \n",
    "\n",
    "# **Topic 4:** Replicating Academic Factors and Measuring Alphas\n",
    "\n",
    "1. We will merge accounting data from Compustat and stock data from CRSP\n",
    "   - I'll use the processed CRSP file we created and saved previously\n",
    "   - By having the merged dataset, we can construct factors such as HML, which is based on sorting stocks into portfolios by their book-to-market ratios\n",
    "\n",
    "\n",
    "2. I provide some general code for constructing factors based on arbitrary signals, such as BE/ME or the signal underneath the momentum factor \n",
    "   - I'll be more careful in my replicating than what I did with the short-term reversals factor\n",
    "   - There are some more details. These are not *that* important in practice, but it is useful to think about them---at the very least, we highlight the fact that many decisions go into constructing trading strategies / factors\n",
    "\n",
    "\n",
    "3. We will then estimate linear regression to assess strategies'/factors' alphas in asset pricing models such as the Capital Asset Pricing model or the Fama-French five-factor model\n",
    "   - I'll use the same Fama-French factors we downloaded and pickled previously\n",
    "   - I'll discuss the meaning of alphas in class, not on this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from io import BytesIO, StringIO\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>Read and process annual fundamentals from Compustat</span>\n",
    "\n",
    "- Compustat is a well-known provider of \"fundamental\" data\n",
    "  - Fundamental in this context means accounting data, that is, income statement and balance sheet information\n",
    "- I downloaded all data for U.S. firms. I look at *annual* reports, which is still the standard in academic literature\n",
    "  - The same ideas of course apply if we use quarterly data or any other data sources besides Compustat\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compustat_url = 'https://dl.dropboxusercontent.com/scl/fi/vd2ci1fw093kbx9375m2z/Compustat_September2023.csv.zip?rlkey=g68xz4deyiq5n7cx5n7q1ma0u'\n",
    "response = requests.get(compustat_url)\n",
    "with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "    with z.open('Compustat_September2023.csv') as f:\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data a bit\n",
    "\n",
    "1. Rename the stock identifier variable to be consistent with the CRSP name\n",
    "2. Compute the book value of equity using the Fama-French definition\n",
    "   - Fama and French (1993) don't provide all the details\n",
    "   - Cohen, Polk, and Vuolteenaho (2003), who were Ph.D. students at Chicago, write:\n",
    "   \n",
    "   \n",
    "   ```\n",
    "    Book equity is defined as the stockholders' equity, plus balance sheet deferred taxes (data item 74) and investment tax credit (data item 208; if available), plus postretirement benefit liabilities (data item 330; if available), minus the book value of preferred stock. Depending on availability, we use redemption (data item 56), liquidation (data item 10), or par value (data item 130) in that order for the book value of preferred stock. Stockholders' equity used in the above formula is calculated as follows. We prefer the stockholders' equity number reported by Moody's or COMPUSTAT (data item 216). If neither one is available, we measure stockholders' equity as the book value of common equity (data item 60), plus the par value of preferred stock. (Note that the preferred stock is added at this stage, because it is later subtracted in the book equity formula.) If common equity is not available, we compute stockholders' equity as the book value of assets (data item 6) minus total liabilities (data item 181), all from COMPUSTAT.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename stock identifier to be consistent with CRSP\n",
    "df = df.rename(columns = {'LPERMNO': 'PERMNO'})\n",
    "df['datadate'] = pd.to_datetime(df['datadate']).dt.to_period('M')\n",
    "df = df.set_index(['PERMNO', 'datadate'])\n",
    "df.tail(3)\n",
    "\n",
    "# compute book value of equity using the Fama-French rules\n",
    "be = df['seq'].combine_first(df['ceq'] + df['pstk']).combine_first(df['at'] - df['lt'])\n",
    "\n",
    "# 1. compute preferred stock\n",
    "pref = df['pstkrv'].combine_first(df['pstkl']).combine_first(df['pstk'])\n",
    "\n",
    "# 2. adjust book value of equity for preferred stock (if exists)\n",
    "pref_not_missing = pref.notnull()\n",
    "be.loc[pref_not_missing] -= pref\n",
    "\n",
    "# 3. investment tax credit (only for fiscal years ending in 1993 or before)\n",
    "df['txditc'] = df['txditc'].replace({np.nan: 0})\n",
    "before_1993 = df.index.get_level_values('datadate') <= '1993-12'\n",
    "be.loc[before_1993] += df.loc[before_1993, 'txditc']\n",
    "\n",
    "df['be'] = be\n",
    "\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only the necessary data\n",
    "\n",
    "- PERMNO and datadate (which are in the index)\n",
    "- at (total assets)\n",
    "- sale (revenue) and cogs (cost of goods sold)\n",
    "- be (book value of equity)\n",
    "- **Note:** I don't use at, sale, and cogs on this notebook, but I'll leave them in for reasons to be discussed in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_vars = ['at', 'sale', 'cogs', 'be']\n",
    "\n",
    "compustat = df[['at', 'sale', 'cogs', 'be']].copy().dropna(how='all')\n",
    "compustat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CRSP data from Topic #3\n",
    "\n",
    "- Also change 'date' to be a period (monthly) to be consistent with Compustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_crsp = pd.read_pickle('data/crsp.pkl')\n",
    "cs_crsp = cs_crsp.reset_index(level='date')\n",
    "cs_crsp['date'] = cs_crsp['date'].dt.to_period('M')\n",
    "cs_crsp = cs_crsp.set_index('date', append=True)\n",
    "cs_crsp.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Compustat data with CRSP\n",
    "\n",
    "- We have monthly stocks returns but *annual* Compustat data\n",
    "- Moreover, Compustat data is reported *not* when the information is available to investors but by fiscal-year ends\n",
    "  - Variable ```datadate``` tells the end date of the fiscal year\n",
    "- **THIS IS A PROBLEM!**\n",
    "  - I need to lag Compustat data appropriately relative to CRSP\n",
    "  - What does this mean? \n",
    "    - Companies announce their earnings with some lag \n",
    "    - A conservative assumption from Fama-French is that accounting data from a fiscal year that ended in year t is available at the end of June in year t+1\n",
    "  - In practice, in live investments, we would of course use whatever is the latest data available\n",
    "  - But when we construct a backtest, we need to consider *what information would have been available to use at the time.*\n",
    "    - We ABSOLUTELY want to avoid lookahead biases\n",
    "- In the code below, I prepare Compustat data for merging as follows:\n",
    "  1. \"reindex\" Compustat data so that we have all months in the data (and not just when we have accounting data for firms)\n",
    "  2. lag data by six months\n",
    "  3. forward-fill data up to 23 months (we allow firm the have one missing annual report before we consider its accounting information too stale)\n",
    "- We now have monthly observations that we can merge directly with CRSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename Compustat datadate variable to 'date'\n",
    "compustat = compustat.rename_axis(index={'datadate': 'date'})\n",
    "\n",
    "# reindex Compustat data to cover all dates seen in either Compustat and CRSP (take the union of dates)\n",
    "cs_index = compustat.index\n",
    "crsp_index = cs_crsp.index\n",
    "combined_index = cs_index.union(crsp_index)\n",
    "compustat = compustat.reindex(combined_index)\n",
    "\n",
    "# there might be some gaps in dates for a firm, which would be a problem for keeping track of how old data are\n",
    "# in the code below, I create an index that covers every month from the first time we see a firm to the last time\n",
    "min_max_dates = compustat.reset_index(level='date').groupby(level='PERMNO').agg(min_date=('date', 'min'), max_date=('date', 'max'))\n",
    "\n",
    "# Create list for multi-index to cover all months for each firm\n",
    "multi_index_list = []\n",
    "\n",
    "for index, row in min_max_dates.iterrows():\n",
    "    months_range = pd.period_range(start=row['min_date'], end=row['max_date'], freq='M')\n",
    "    for month in months_range:\n",
    "        multi_index_list.append((index, month))\n",
    "\n",
    "new_index = pd.MultiIndex.from_tuples(multi_index_list, names=['PERMNO', 'date'])\n",
    "compustat = compustat.reindex(new_index)\n",
    "\n",
    "print('Data without shifting and ffill')\n",
    "display(compustat.tail(25))\n",
    "\n",
    "compustat = compustat.groupby(level='PERMNO').shift(6).ffill(limit=23)\n",
    "print('Data after shifting and ffill')\n",
    "display(compustat.tail(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data with CRSP and pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_crsp = cs_crsp.merge(compustat, left_index=True, right_index=True, how='left')\n",
    "cs_crsp.to_pickle('data/cs_crsp.pkl')\n",
    "cs_crsp.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define two functions for replicating academic factors\n",
    "\n",
    "## Function 1: Assign stocks into portfolios based breakpoints\n",
    "\n",
    "- I let the function take in a bunch of inputs so that we can be flexible to create all kinds of factors\n",
    "Note:\n",
    "\n",
    "- Fama and French update their portfolio sorts only in June\n",
    "- If we do similar \"annual\" sorts:\n",
    "  - Set non-June assignments to zero\n",
    "  - Copy previous groups assignment forward to fill non-June months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_sort(df=None, col=None, nyse=True, percentiles=None, id_col=None, annual=True):\n",
    "    \n",
    "    if nyse:\n",
    "        sortvar = df.loc[df['EXCHCD']==1, col]\n",
    "    else:\n",
    "        sortvar = df[col]\n",
    "    \n",
    "    df[id_col] = np.nan\n",
    "    group_id = 1\n",
    "    grp = sortvar.dropna().groupby(level='date')\n",
    "    \n",
    "    for pct in percentiles:\n",
    "        breakpoint = grp.apply(lambda x: np.percentile(x, pct))\n",
    "        breakpoint.name = 'breakpoint'\n",
    "        df_merged = df.merge(breakpoint, left_on='date', right_index=True, how='left')\n",
    "        assigned = df_merged[id_col].isnull() & (df_merged[col] <= df_merged['breakpoint'])         \n",
    "        df.loc[assigned[assigned].index, id_col] = group_id\n",
    "        group_id += 1\n",
    "    \n",
    "    # assign firms to to the right from the last breakpoint into a group \n",
    "    assigned = df_merged[id_col].isnull() & (df_merged[col] > df_merged['breakpoint']) \n",
    "    df.loc[assigned[assigned].index, id_col] = group_id\n",
    "    \n",
    "    if annual:\n",
    "        nonJune = df.index.get_level_values(level='date').month != 6\n",
    "        df.loc[nonJune, id_col] = np.nan\n",
    "        df[id_col] = df.groupby(level='PERMNO')[id_col].ffill(limit=11)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2: Assign compute portfolio returns for sorts define in 'sort_groups'\n",
    "\n",
    "This function computes value-weighted portfolio returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_portfolio_returns(df=None, sort_groups=None):\n",
    "    \n",
    "    display(df.shape)\n",
    "\n",
    "    df['retnm'] = df['ret'].groupby(level='PERMNO').shift(-1)\n",
    "    df['me_x_retnm'] = df['me'] * df['retnm']\n",
    "\n",
    "    # require me, sort variables, and return next month\n",
    "    ok = df['me'].notnull()\n",
    "    for required_var in ['retnm'] + sort_variables:\n",
    "        ok = ok & df[required_var].notnull()\n",
    "    df = df[ok]\n",
    "\n",
    "    display(df.shape)\n",
    "\n",
    "    sums = df.reset_index().groupby(by=['date'] + sort_groups)[['me', 'me_x_retnm']].sum()\n",
    "    portfolio_returns = sums['me_x_retnm'] / sums['me']\n",
    "    portfolio_returns = portfolio_returns.unstack(level=sort_groups)\n",
    "    \n",
    "    # because we used return as of NEXT MONTH, undo the timing so that the date in the index corresponds to the return realization\n",
    "    portfolio_returns = portfolio_returns.shift(1)\n",
    "    \n",
    "    return portfolio_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3: Analysis function for measuring Sharpe ratios \n",
    "\n",
    "- This is from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_returns(r=None, name=None, start_date='1962-05', end_date='2023-09'):\n",
    "    r = r.loc[start_date:end_date]\n",
    "    ir = np.sqrt(12) * r.mean() / r.std()\n",
    "    print(f'Analysis of a strategy: \"{name}\"')\n",
    "    print(f'Start: {start_date}, End: {end_date}')\n",
    "    print(f'Sharpe ratio: {ir:.2f}')\n",
    "    r.cumsum().plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the return on the Fama-French value factor, HML\n",
    "\n",
    "- With the functions I defined above, I just need to define\n",
    "  - What variable is our signal (beme)\n",
    "  - What percentiles do we use for size and book-to-market sorts? (50 and 30-70)\n",
    "  - Do we use NYSE breakpoints? (yes)\n",
    "  - Do we sort only annually at the end of June? (yes)\n",
    "\n",
    "\n",
    "Fama and French's portfolio is construct by assigning stocks into six portfolios: small-value, small-neutral, small-growth, big-value,...\n",
    "\n",
    "\n",
    "The return on HML is then:\n",
    "\n",
    "HML = (1/2) * (small-value + big-value) - (1/2) * (small-growth + big-growth)\n",
    "\n",
    "#### Notes on the code below:\n",
    "\n",
    "- I create a list ```sort_variables``` to indicate by which variables I sort\n",
    "- I create a dictionary percentiles (with the sort variables as keys) to indicate what breakpoints I want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the original data (we will modify it)\n",
    "cs_crsp = pd.read_pickle('data/cs_crsp.pkl')\n",
    "\n",
    "# construct BE/ME - set firms with negative BEs to missing\n",
    "cs_crsp['beme'] = cs_crsp['be'] / cs_crsp['me']\n",
    "negative_be = cs_crsp['be'] < 0\n",
    "cs_crsp.loc[negative_be, 'beme'] = np.nan\n",
    "\n",
    "sort_variables = ['me', 'beme']\n",
    "percentiles = {'me': [50],\n",
    "              'beme': [30, 70]}\n",
    "\n",
    "for sortvar in sort_variables:\n",
    "    cs_crsp = portfolio_sort(df=cs_crsp, col=sortvar, nyse=True, percentiles=percentiles[sortvar], id_col=sortvar + '_group', annual=True)\n",
    "    \n",
    "sort_groups = [sortvar + '_group' for sortvar in sort_variables]\n",
    "\n",
    "portfolio_returns = compute_portfolio_returns(cs_crsp, sort_groups=sort_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hml = portfolio_returns.loc[:,[(1,3), (2,3)]].mean(axis=1) - portfolio_returns.loc[:,[(1,1), (2,1)]].mean(axis=1)\n",
    "hml.name = 'hml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_returns(hml, 'Our HML', end_date='2007-06')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Fama and French data from the pickle file we created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_data = pd.read_pickle('data/ff_data.pkl')\n",
    "ff_data.index = ff_data.index.to_period('M')\n",
    "ff_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.concat([hml, ff_data], axis=1).dropna()\n",
    "merged_data.corr().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:red'>Our replication of HML looks pretty reasonable</span>\n",
    "\n",
    "A correlation of **0.93**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a momentum factor\n",
    "\n",
    "Momentum in stock returns is typically defined by sorting on stocks returns from month t-12 to month t-2 \n",
    "\n",
    "I construct the equivalent of Fama and French's UMD factor below\n",
    "\n",
    "Note: This factor is rebalanced monthly, and so I set annual=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the original data (we will modify it)\n",
    "cs_crsp = pd.read_pickle('data/cs_crsp.pkl')\n",
    "\n",
    "cs_crsp['r12_2'] = cs_crsp['ret'].rolling(window=11).sum(min_count=6).shift(1)\n",
    "\n",
    "sort_variables = ['me', 'r12_2']\n",
    "percentiles = {'me': [50],\n",
    "              'r12_2': [30, 70]}\n",
    "\n",
    "for sortvar in sort_variables:\n",
    "    cs_crsp = portfolio_sort(df=cs_crsp, col=sortvar, nyse=True, percentiles=percentiles[sortvar], id_col=sortvar + '_group', annual=False)\n",
    "    \n",
    "sort_groups = [sortvar + '_group' for sortvar in sort_variables]\n",
    "\n",
    "portfolio_returns = compute_portfolio_returns(cs_crsp, sort_groups=sort_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umd = portfolio_returns.loc[:,[(1,3), (2,3)]].mean(axis=1) - portfolio_returns.loc[:,[(1,1), (2,1)]].mean(axis=1)\n",
    "umd = umd.shift(1)\n",
    "umd.name = 'umd'\n",
    "analyze_returns(umd, 'Momentum Factor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring alphas\n",
    "\n",
    "We measure alphas by\n",
    "\n",
    "```Running a linear regression of strategy returns against some factors```\n",
    "\n",
    "- In CAPM there is only one factor on the RHS: MKTRF\n",
    "- In the Fama-French three-factor model, there are three factors: MKTRF, SMB, and HML\n",
    "- In the Fama-French five-factor model, there are three factors: MKTRF, SMB, HML, RMW, and CMA\n",
    "\n",
    "**Alphas** measure stocks', managers', or strategies' *abnormal* returns\n",
    "\n",
    "That is, how profitable an investment is when we 'expunge' from returns any exposures to the factors of the factor model\n",
    "\n",
    "Notes:\n",
    "\n",
    "- I use statsmodels.api for running the linear regression\n",
    "  - This is a well-known (and well-maintained) package\n",
    "  - The benefit of this package is that it gives a nice, easy summary of the results\n",
    "- Other packages, such as sklearn, are better for estimating more complex models\n",
    "  - They don't provide similar summary statistics -- because such summary statistics are often hard to compute for more complicated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "regression_data = pd.concat([umd, ff_data], axis=1).dropna()\n",
    "\n",
    "y = regression_data['umd']\n",
    "X = regression_data[['Mkt-RF', 'SMB', 'HML']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Create a model. This is an OBJECT that comes with methods. We are NOT estimating the model yet, just creating it.\n",
    "model = sm.OLS(y, X)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute abnormal return and analyze it\n",
    "\n",
    "- This is the return that an investor who trades \"pure\" momentum would get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_umd = results.params['const'] + results.resid\n",
    "analyze_returns(abnormal_umd, 'UMD (abnormal return)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'umd': umd, 'umd (alpha)': abnormal_umd})\n",
    "df.cumsum().plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "What if we repeat this computation for our HML factor?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
